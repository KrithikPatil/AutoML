import pandas as pd
import numpy as np
from sklearn.datasets import load_iris
from dotenv import load_dotenv
import os
import ollama
from io import StringIO
from sdv.single_table import GaussianCopulaSynthesizer  # or use GaussianCopula, TVAE
from sdv.metadata import SingleTableMetadata

# Load environment variables
load_dotenv()

CSV_PROMPT_TEMPLATE = """
You are a synthetic dataset generator.

Based on the task description and dataset requirements below, generate a synthetic dataset in raw CSV format with 10 sample rows and a header. You are allowed to assume and include **additional relevant features** that make sense for the given machine learning task. The dataset should be logically consistent and include realistic values.

‚ùó Important Instructions:
- Output only raw CSV (no markdown, no explanations, no headers like "CSV:", no quotes around the entire CSV block).
- Include a header row.
- Ensure data types are appropriate (e.g., integers for age, floats for charges, binary for yes/no fields, etc.).

User Prompt: {task_description}
Dataset Requirements: {dataset_requirements}

Output:
"""

class DatasetAgent:
    def __init__(self, task_description, dataset_requirements, num_samples):
        self.task_description = task_description
        self.dataset_requirements = dataset_requirements
        self.model = os.getenv("OLLAMA_MODEL", "mistral")
        self.n_samples = num_samples

    def generate_csv_from_llm(self):
        try:
            prompt = CSV_PROMPT_TEMPLATE.format(
                task_description=self.task_description,
                dataset_requirements=self.dataset_requirements
            )
            response = ollama.generate(
                model=self.model,
                prompt=prompt,
                options={"num_predict": 800}  # More tokens for bigger response
            )
            csv_text = response["response"].strip()
            print("Raw CSV Response:\n", csv_text)

            df = pd.read_csv(StringIO(csv_text))
            return df, "CSV-based synthetic dataset generated by LLM."
        except Exception as e:
            print(f"CSV generation failed: {e}, falling back to default Iris dataset.")
            return self.load_default_dataset(f"Default Iris dataset used due to error: {str(e)}")

    def enlarge_dataset_with_sdv(self, df, n_samples):
        try:
            print(f"Training SDV model to enlarge to {n_samples} rows...")
            metadata = SingleTableMetadata()
            metadata.detect_from_dataframe(data=df)
            model = GaussianCopulaSynthesizer(metadata)  # or TVAE/GaussianCopula
            model.fit(df)
            synthetic_df = model.sample(n_samples)
            return synthetic_df, f"Dataset enlarged to {n_samples} rows using SDV (CTGAN)."
        except Exception as e:
            print(f"SDV enlargement failed: {e}, returning original dataset.")
            return df, f"Failed to enlarge dataset: {str(e)}"

    def load_default_dataset(self, reason="Loaded default Iris dataset."):
        iris = load_iris()
        X = pd.DataFrame(iris.data, columns=iris.feature_names)
        y = pd.Series(iris.target, name="target")
        df = pd.concat([X, y], axis=1)
        return df, reason

    def run(self):
        initial_df, reason = self.generate_csv_from_llm()
        enlarged_df, enlargement_reason = self.enlarge_dataset_with_sdv(initial_df, self.n_samples)

        os.makedirs("dataset", exist_ok=True)
        csv_path = os.path.join("dataset", "generated_dataset.csv")
        enlarged_df.to_csv(csv_path, index=False)
        print(f"Saved dataset to {csv_path}")

        X = enlarged_df.iloc[:, :-1]
        y = enlarged_df.iloc[:, -1]
        return X, y, f"{reason} | {enlargement_reason}"
